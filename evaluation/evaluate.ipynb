{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "VECTOR_DIR = \"data/vectorstore\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "store = Chroma(\n",
    "    persist_directory=VECTOR_DIR,\n",
    "    embedding_function=hf_embeddings,\n",
    ")\n",
    "retriever = store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "\n",
    "def rag(question: str):\n",
    "    docs = retriever.invoke(question)\n",
    "    contexts = [d.page_content for d in docs]\n",
    "    return {\"question\": question,\n",
    "            \"contexts\": contexts,\n",
    "            \"metadata\": [d.metadata for d in docs]}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful assistant. \"\n",
    "     \"답변은 질문에 입력된 언어로 하며, 아래 제공된 contexts에 있는 정보만 사용해. \"\n",
    "     \"contexts에 없는 내용은 추정하지 말고 '제공된 자료로는 확인할 수 없습니다'라고 답해.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\n\"\n",
    "     \"Contexts (use ONLY this info):\\n{ctx}\\n\\n\")\n",
    "])\n",
    "\n",
    "def generate_answer(question: str, contexts: list[str]) -> str:\n",
    "    if not contexts:\n",
    "        return \"제공된 자료로는 확인할 수 없습니다.\"\n",
    "    ctx_joined = \"\\n\".join(f\"- {c}\" for c in contexts)\n",
    "    messages = prompt.format_messages(question=question, ctx=ctx_joined)\n",
    "    resp = llm.invoke(messages)\n",
    "    return resp.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# 모든 scenario_*.jsonl 파일을 찾아서 DATASET 리스트로 만듦\n",
    "DATASET_PATHS = sorted(glob.glob(\"data/scenario_*.jsonl\"))\n",
    "OUTDIR  = Path(\"results\")\n",
    "\n",
    "def load_gold_rows(paths: list[str]):\n",
    "    rows = []\n",
    "    for path in paths:\n",
    "        path = Path(path)\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                item = json.loads(line)\n",
    "                rows.append(item)\n",
    "    return rows\n",
    "\n",
    "\n",
    "gold = load_gold_rows(DATASET_PATHS)\n",
    "print(f\"Loaded {len(gold)} questions from {len(DATASET_PATHS)} files.\")\n",
    "\n",
    "# 1) 질문별로 retrieval + generation 실행\n",
    "preds = []\n",
    "for item in gold:\n",
    "    q  = item[\"question\"]\n",
    "    gt = item[\"ground_truth\"] \n",
    "    r  = rag(q)                # {\"question\",\"contexts\",\"metadata\"}\n",
    "    ctxs = r.get(\"contexts\", [])\n",
    "    ans  = generate_answer(q, ctxs)\n",
    "    preds.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"contexts\": ctxs,\n",
    "        \"ground_truth\": gt,\n",
    "    })\n",
    "\n",
    "# 2) HuggingFace Dataset 변환\n",
    "df = pd.DataFrame(preds)\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "# 3) RAGAS 메트릭 계산\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    ")\n",
    "\n",
    "result_df = result.to_pandas()\n",
    "display(result_df)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Calculate mean scores for each metric\n",
    "result_df['faithfulness_mean'] = result_df['faithfulness'].mean()\n",
    "result_df['answer_relevancy_mean'] = result_df['answer_relevancy'].mean()\n",
    "result_df['context_precision_mean'] = result_df['context_precision'].mean()\n",
    "result_df['context_recall_mean'] = result_df['context_recall'].mean()\n",
    "\n",
    "csv_filename = f\"{current_time}.csv\"\n",
    "csv_path = OUTDIR / csv_filename\n",
    "result_df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print(f\"Results saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
